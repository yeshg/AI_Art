{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports and basic notebook setup\n",
    "from cStringIO import StringIO\n",
    "import numpy as np\n",
    "import os,re,random\n",
    "import scipy.ndimage as nd\n",
    "import PIL.Image\n",
    "import PIL.ImageEnhance\n",
    "import sys\n",
    "from IPython.display import clear_output, Image, display\n",
    "from scipy.misc import imresize\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pycaffe_root = \"/home/s/caffe/python\"\n",
    "sys.path.insert(0, pycaffe_root)\n",
    "import caffe\n",
    "\n",
    "model_name = \"GoogLeNet\"\n",
    "model_path = '/home/s/caffe/models/bvlc_googlenet/'\n",
    "net_fn   = './deploy_googlenet_updated.prototxt'\n",
    "param_fn = model_path + 'bvlc_googlenet.caffemodel'\n",
    "mean = np.float32([104.0, 117.0, 123.0])\n",
    "\n",
    "net = caffe.Classifier(net_fn, param_fn,\n",
    "                       mean = mean, # ImageNet mean, training set dependent\n",
    "                       channel_swap = (2,1,0)) # the reference model has channels in BGR order instead of RGB\n",
    "\n",
    "# a couple of utility functions for converting to and from Caffe's input image layout\n",
    "def preprocess(net, img):\n",
    "    return np.float32(np.rollaxis(img, 2)[::-1]) - net.transformer.mean['data']\n",
    "def deprocess(net, img):\n",
    "    return np.dstack((img + net.transformer.mean['data'])[::-1])\n",
    "\n",
    "def blur(img, sigma):\n",
    "    if sigma > 0:\n",
    "        img[0] = nd.filters.gaussian_filter(img[0], sigma, order=0)\n",
    "        img[1] = nd.filters.gaussian_filter(img[1], sigma, order=0)\n",
    "        img[2] = nd.filters.gaussian_filter(img[2], sigma, order=0)\n",
    "    return img\n",
    "\n",
    "def bilateralFilter_and_Blur(img, sigma):\n",
    "    img[0] = cv2.bilateralFilter(nd.filters.gaussian_filter(img[0], sigma, order=0), 5, 5, 5)\n",
    "    img[1] = cv2.bilateralFilter(nd.filters.gaussian_filter(img[1], sigma, order=0), 5, 5, 5)\n",
    "    img[2] = cv2.bilateralFilter(nd.filters.gaussian_filter(img[2], sigma, order=0), 5, 5, 5)\n",
    "    return img\n",
    "\n",
    "def no_blur(img, sigma):\n",
    "    # do nothing\n",
    "    return img\n",
    "\n",
    "def showarray(a, f, fmt='jpeg'):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    #display(Image(data=f.getvalue()))\n",
    "    cv2.imshow('Main', cv2.cvtColor(np.array(PIL.Image.open(f).convert('RGB')), cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the main gradient ascent functions. Based on the [deepdream code](https://github.com/google/deepdream/blob/master/dream.ipynb) published by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_step(net, step_size=1.5, end='inception_4c/output', clip=True, focus=None, sigma=None):\n",
    "    '''Basic gradient ascent step.'''\n",
    "\n",
    "    src = net.blobs['data'] # input image is stored in Net's 'data' blob\n",
    "    \n",
    "    dst = net.blobs[end]\n",
    "    net.forward(end=end)\n",
    "\n",
    "    one_hot = np.zeros_like(dst.data)\n",
    "    one_hot.flat[focus] = 1.\n",
    "    dst.diff[:] = one_hot\n",
    "\n",
    "    net.backward(start=end)\n",
    "    g = src.diff[0]\n",
    "    \n",
    "    src.data[:] += step_size/np.abs(g).mean() * g\n",
    "\n",
    "    if clip:\n",
    "        bias = net.transformer.mean['data']\n",
    "        src.data[:] = np.clip(src.data, -bias, 255-bias) \n",
    "    \n",
    "    src.data[0] = bilateralFilter_and_Blur(src.data[0], sigma)  # Approach #3 (blur + bilateralFilter)\n",
    "    # src.data[0] = blur(src.data[0], sigma)                    # Approach #3 (only blur)\n",
    "    # src.data[0] = no_blur(src.data[0], sigma)                 # Approach #2\n",
    "    \n",
    "    # reset objective for next step\n",
    "    dst.diff.fill(0.)\n",
    "\n",
    "def deepdraw(net, base_img, octaves, random_crop=True, visualize=True, focus=None,\n",
    "    clip=True, **step_params):\n",
    "    \n",
    "    # prepare base image\n",
    "    image = preprocess(net, base_img)\n",
    "    \n",
    "    # get input dimensions from net\n",
    "    w = net.blobs['data'].width\n",
    "    h = net.blobs['data'].height\n",
    "    \n",
    "    print \"starting drawing\"\n",
    "    src = net.blobs['data']\n",
    "    src.reshape(1,3,h,w)                                    # reshape the network's input image size\n",
    "    for e,o in enumerate(octaves):                          # scale up image if specified in octaves\n",
    "        if 'scale' in o:\n",
    "            # resize by o['scale'] if it exists\n",
    "            image = nd.zoom(image, (1,o['scale'],o['scale']))\n",
    "        _,imw,imh = image.shape\n",
    "        \n",
    "        # select layer\n",
    "        layer = o['layer']\n",
    "\n",
    "        for i in xrange(o['iter_n']):\n",
    "            if imw > w:\n",
    "                if random_crop:\n",
    "                    # randomly select a crop \n",
    "                    mid_x = (imw-w)/2.\n",
    "                    width_x = imw-w\n",
    "                    ox = np.random.normal(mid_x, width_x*0.3, 1)\n",
    "                    ox = int(np.clip(ox,0,imw-w))\n",
    "                    mid_y = (imh-h)/2.\n",
    "                    width_y = imh-h\n",
    "                    oy = np.random.normal(mid_y, width_y*0.3, 1)\n",
    "                    oy = int(np.clip(oy,0,imh-h))\n",
    "                    # insert the crop into src.data[0]\n",
    "                    src.data[0] = image[:,ox:ox+w,oy:oy+h]\n",
    "                else:\n",
    "                    ox = (imw-w)/2.\n",
    "                    oy = (imh-h)/2.\n",
    "                    src.data[0] = image[:,ox:ox+w,oy:oy+h]\n",
    "            else:\n",
    "                ox = 0\n",
    "                oy = 0\n",
    "                src.data[0] = image.copy()\n",
    "\n",
    "            sigma = o['start_sigma'] + ((o['end_sigma'] - o['start_sigma']) * i) / o['iter_n']\n",
    "            step_size = o['start_step_size'] + ((o['end_step_size'] - o['start_step_size']) * i) / o['iter_n']\n",
    "            \n",
    "            make_step(net, end=layer, clip=clip, focus=focus, \n",
    "                      sigma=sigma, step_size=step_size)\n",
    "            \n",
    "            if visualize:\n",
    "                vis = deprocess(net, src.data[0])\n",
    "                if not clip: # adjust image contrast if clipping is disabled\n",
    "                    vis = vis*(255.0/np.percentile(vis, 99.98))\n",
    "                if i % 1 == 0:\n",
    "                    showarray(vis,\"./filename\"+str(i)+\".jpg\")\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print 'finished step %d in octave %d' % (i,e)\n",
    "            \n",
    "            # insert modified image back into original image (if necessary)\n",
    "            image[:,ox:ox+w,oy:oy+h] = src.data[0]\n",
    "        \n",
    "        print \"octave %d image:\" % e\n",
    "        showarray(deprocess(net, image),\"./octave_\"+str(e)+\".jpg\")\n",
    "            \n",
    "    # returning the resulting image\n",
    "    return np.uint8(deprocess(net, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the class visualizations\n",
    "\n",
    "* ```layer``` : which layer to optimize\n",
    "* ```iter_n``` : how many iterations\n",
    "* ```scale``` : by what factor (if any) to scale up the base image before proceeding\n",
    "* ```start_sigma``` : the initial radius of the gaussian blur\n",
    "* ```end_sigma``` : the final radius of the gaussian blur\n",
    "* ```start_step_size``` : the initial step size of the gradient ascent\n",
    "* ```end_step_size``` : the final step size of the gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "format": "row",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting drawing\n",
      "finished step 0 in octave 0\n",
      "finished step 10 in octave 0\n",
      "finished step 20 in octave 0\n",
      "finished step 30 in octave 0\n",
      "finished step 40 in octave 0\n",
      "finished step 50 in octave 0\n",
      "finished step 60 in octave 0\n",
      "finished step 70 in octave 0\n",
      "finished step 80 in octave 0\n",
      "finished step 90 in octave 0\n",
      "finished step 100 in octave 0\n",
      "finished step 110 in octave 0\n",
      "finished step 120 in octave 0\n",
      "finished step 130 in octave 0\n",
      "finished step 140 in octave 0\n",
      "finished step 150 in octave 0\n",
      "finished step 160 in octave 0\n",
      "finished step 170 in octave 0\n",
      "finished step 180 in octave 0\n",
      "octave 0 image:\n",
      "finished step 0 in octave 1\n",
      "finished step 10 in octave 1\n",
      "finished step 20 in octave 1\n",
      "finished step 30 in octave 1\n",
      "finished step 40 in octave 1\n",
      "finished step 50 in octave 1\n",
      "finished step 60 in octave 1\n",
      "finished step 70 in octave 1\n",
      "finished step 80 in octave 1\n",
      "finished step 90 in octave 1\n",
      "finished step 100 in octave 1\n",
      "finished step 110 in octave 1\n",
      "finished step 120 in octave 1\n",
      "finished step 130 in octave 1\n",
      "finished step 140 in octave 1\n",
      "octave 1 image:\n",
      "finished step 0 in octave 2\n",
      "finished step 10 in octave 2\n",
      "finished step 20 in octave 2\n",
      "finished step 30 in octave 2\n",
      "finished step 40 in octave 2\n",
      "finished step 50 in octave 2\n",
      "finished step 60 in octave 2\n",
      "finished step 70 in octave 2\n",
      "finished step 80 in octave 2\n",
      "finished step 90 in octave 2\n",
      "finished step 100 in octave 2\n",
      "finished step 110 in octave 2\n",
      "finished step 120 in octave 2\n",
      "finished step 130 in octave 2\n",
      "finished step 140 in octave 2\n",
      "octave 2 image:\n",
      "finished step 0 in octave 3\n",
      "octave 3 image:\n"
     ]
    }
   ],
   "source": [
    "cv2.startWindowThread()\n",
    "\n",
    "cv2.namedWindow('Main', cv2.WINDOW_NORMAL)\n",
    "\n",
    "\n",
    "octaves = [\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'iter_n':190,\n",
    "        'start_sigma':2.5,\n",
    "        'end_sigma':0.78,\n",
    "        'start_step_size':11.,\n",
    "        'end_step_size':11.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':150,\n",
    "        'start_sigma':0.78*1.2,\n",
    "        'end_sigma':0.78,\n",
    "        'start_step_size':6.,\n",
    "        'end_step_size':6.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss2/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':150,\n",
    "        'start_sigma':0.78*1.2,\n",
    "        'end_sigma':0.44,\n",
    "        'start_step_size':6.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss1/classifier',\n",
    "        'iter_n':10,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    }\n",
    "]\n",
    "\n",
    "# get original image input size from network\n",
    "original_w = net.blobs['data'].width\n",
    "original_h = net.blobs['data'].height\n",
    "# the background color of the initial image\n",
    "# background_color = np.float32([28.0, 84.0, 122.0]) # This is blue \n",
    "# background_color = np.float32([54.0, 42.0, 77.0]) # This is dark purple \n",
    "background_color = np.float32([10, 90, 140])\n",
    "\n",
    "imagenet_class = 79\n",
    "gen_image = np.random.normal(background_color, 8, (original_w, original_h, 3)) # create image of random noise with dimensions of input image\n",
    "\n",
    "showarray(gen_image,\"./octave_\"+\".jpg\")\n",
    "\n",
    "gen_image = deepdraw(net, gen_image, octaves, focus=imagenet_class, random_crop=True, visualize=True)\n",
    "img_fn = '_'.join([model_name, \"deepdraw_denoised\", str(imagenet_class)+'.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "octaves = [\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'iter_n':190,\n",
    "        'start_sigma':2.5,\n",
    "        'end_sigma':0.78,\n",
    "        'start_step_size':11.,\n",
    "        'end_step_size':11.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':150,\n",
    "        'start_sigma':0.78*1.2,\n",
    "        'end_sigma':0.78,\n",
    "        'start_step_size':6.,\n",
    "        'end_step_size':6.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss2/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':150,\n",
    "        'start_sigma':0.78*1.2,\n",
    "        'end_sigma':0.44,\n",
    "        'start_step_size':6.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss1/classifier',\n",
    "        'iter_n':10,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    }\n",
    "]\n",
    "\n",
    "# get original image input size from network\n",
    "original_w = net.blobs['data'].width\n",
    "original_h = net.blobs['data'].height\n",
    "# the background color of the initial image\n",
    "# background_color = np.float32([28.0, 84.0, 122.0]) # This is blue \n",
    "# background_color = np.float32([54.0, 42.0, 77.0]) # This is dark purple \n",
    "background_color = np.float32([10, 90, 140])\n",
    "\n",
    "imagenet_class = 79\n",
    "gen_image = np.random.normal(background_color, 8, (original_w, original_h, 3)) # create image of random noise with dimensions of input image\n",
    "gen_image = deepdraw(net, gen_image, octaves, focus=imagenet_class, random_crop=True)\n",
    "img_fn = '_'.join([model_name, \"deepdraw_denoised\", str(imagenet_class)+'.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting drawing\n",
      "finished step 0 in octave 0\n",
      "finished step 10 in octave 0\n",
      "finished step 20 in octave 0\n",
      "finished step 30 in octave 0\n",
      "finished step 40 in octave 0\n",
      "finished step 50 in octave 0\n",
      "finished step 60 in octave 0\n",
      "finished step 70 in octave 0\n",
      "finished step 80 in octave 0\n",
      "finished step 90 in octave 0\n",
      "finished step 100 in octave 0\n",
      "finished step 110 in octave 0\n",
      "finished step 120 in octave 0\n",
      "finished step 130 in octave 0\n",
      "finished step 140 in octave 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ebc8cbbccaec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mimagenet_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m79\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0mgen_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moriginal_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create image of random noise with dimensions of input image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mgen_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moctaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimagenet_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_crop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0mimg_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"deepdraw_denoised\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagenet_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-b3c547eb3441>\u001b[0m in \u001b[0;36mdeepdraw\u001b[0;34m(net, base_img, octaves, random_crop, visualize, focus, clip, **step_params)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             make_step(net, end=layer, clip=clip, focus=focus, \n\u001b[0;32m---> 78\u001b[0;31m                       sigma=sigma, step_size=step_size)\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-b3c547eb3441>\u001b[0m in \u001b[0;36mmake_step\u001b[0;34m(net, step_size, end, clip, focus, sigma)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/s/caffe/python/caffe/pycaffe.pyc\u001b[0m in \u001b[0;36m_Net_forward\u001b[0;34m(self, blobs, start, end, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# Unpack blobs to extract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# these octaves are where bulk of tuning occurs, they determine gradient ascent steps\n",
    "octaves = [\n",
    "    {\n",
    "        'layer':'loss3/classifier',   # layer to perform image updates from\n",
    "        'iter_n':190,                 # number of times to perform image update\n",
    "        'start_sigma':2.5,            # Gradually reduce gaussian blur as recommended by \n",
    "        'end_sigma':0.78,             #   https://github.com/kylemcdonald/deepdream\n",
    "        'start_step_size':11.,        # Gradually change step size of gradient ascent as recommended by \n",
    "        'end_step_size':11.           #   http://www.auduno.com/2015/07/29/visualizing-googlenet-classes/\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':150,\n",
    "        'start_sigma':0.78*1.2,\n",
    "        'end_sigma':0.78,\n",
    "        'start_step_size':6.,\n",
    "        'end_step_size':6.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':70,\n",
    "        'start_sigma':0.78*1.2,\n",
    "        'end_sigma':0.44,\n",
    "        'start_step_size':6.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':50,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':30,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss2/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':150,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss2/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':70,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss2/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':40,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss2/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':30,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss2/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':20,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss2/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':20,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss1/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':20,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss1/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':20,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss1/classifier',\n",
    "        'iter_n':30,\n",
    "        'start_sigma':0.44,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    }\n",
    "]\n",
    "\n",
    "# get original image input size from network\n",
    "original_w = net.blobs['data'].width\n",
    "original_h = net.blobs['data'].height\n",
    "# the background color of the initial image\n",
    "# background_color = np.float32([28.0, 84.0, 122.0]) # This is blue \n",
    "background_color = np.float32([70, 83, 82])\n",
    "\n",
    "imagenet_class = 79\n",
    "gen_image = np.random.normal(background_color, 8, (original_w, original_h, 3)) # create image of random noise with dimensions of input image\n",
    "gen_image = deepdraw(net, gen_image, octaves, focus=imagenet_class, random_crop=True)\n",
    "img_fn = '_'.join([model_name, \"deepdraw_denoised\", str(imagenet_class)+'.png'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for i in range(0,100): # Automate image visualization and writing to disk\n",
    "    imagenet_class = i\n",
    "    gen_image = np.random.normal(background_color, 8, (original_w, original_h, 3)) # create image of random noise with dimensions of input image\n",
    "    gen_image = deepdraw(net, gen_image, octaves, focus=imagenet_class, random_crop=True)\n",
    "    img_fn = '_'.join([model_name, \"deepdraw_denoised\", str(imagenet_class)+'.png'])\n",
    "    PIL.Image.fromarray(cv2.bilateralFilter(np.uint8(gen_image), -1, 25, 6)).save('./' + img_fn) # Very strong bilateral filter after visualization\n",
    "    # PIL.Image.fromarray(np.uint8(gen_image)).save('./' + img_fn)                               # This \"cartoonifies\" the image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PIL.Image.fromarray(cv2.bilateralFilter(np.uint8(gen_image), -1, 25, 6)).save('./' + img_fn) # Very strong bilateral filter after visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Less scaling and more iterations gives lower resolution but more coherent image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "octaves = [\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'iter_n':190,\n",
    "        'start_sigma':2.5,\n",
    "        'end_sigma':0.78,\n",
    "        'start_step_size':11.,\n",
    "        'end_step_size':11.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':450,\n",
    "        'start_sigma':0.78*1.2,\n",
    "        'end_sigma':0.40,\n",
    "        'start_step_size':6.,\n",
    "        'end_step_size':3.\n",
    "    }\n",
    "]\n",
    "\n",
    "# get original image input size from network\n",
    "original_w = net.blobs['data'].width\n",
    "original_h = net.blobs['data'].height\n",
    "# the background color of the initial image\n",
    "# background_color = np.float32([28.0, 84.0, 122.0]) # This is blue \n",
    "background_color = np.float32([70, 83, 82])\n",
    "\n",
    "imagenet_class = 79\n",
    "gen_image = np.random.normal(background_color, 8, (original_w, original_h, 3))\n",
    "gen_image = deepdraw(net, gen_image, octaves, focus=imagenet_class, \n",
    "                 random_crop=True)\n",
    "\n",
    "#img_fn = '_'.join([model_name, \"deepdraw\", str(imagenet_class)+'.png'])\n",
    "#PIL.Image.fromarray(np.uint8(gen_image)).save('./' + img_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choices below give images that are not as large as in first set of automation, but are more coherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "octaves = [\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        \n",
    "        'iter_n':190,\n",
    "        'start_sigma':2.5,\n",
    "        'end_sigma':0.78,\n",
    "        'start_step_size':11.,\n",
    "        'end_step_size':11.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss3/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':100,\n",
    "        'start_sigma':0.78*1.2,\n",
    "        'end_sigma':0.65,\n",
    "        'start_step_size':6.,\n",
    "        'end_step_size':6.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss2/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':90,\n",
    "        'start_sigma':0.78*1,\n",
    "        'end_sigma':0.55,\n",
    "        'start_step_size':6.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss2/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':60,\n",
    "        'start_sigma':0.78*0.8,\n",
    "        'end_sigma':0.45,\n",
    "        'start_step_size':6.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss1/classifier',\n",
    "        'scale':1.2,\n",
    "        'iter_n':10,\n",
    "        'start_sigma':0.45,\n",
    "        'end_sigma':0.304,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':3.\n",
    "    },\n",
    "    {\n",
    "        'layer':'loss1/classifier',\n",
    "        'iter_n':5,\n",
    "        'start_sigma':0.304,\n",
    "        'end_sigma':0.2,\n",
    "        'start_step_size':3.,\n",
    "        'end_step_size':1.\n",
    "    }\n",
    "]\n",
    "\n",
    "background_color = np.float32([85.0, 98.0, 112.0]) # \"Slate\" color -- tends to not work as well as blue with bilateral filter\n",
    "\n",
    "for i in range(901,1000):\n",
    "    imagenet_class = i\n",
    "    gen_image = np.random.normal(background_color, 8, (original_w, original_h, 3))\n",
    "    gen_image = deepdraw(net, gen_image, octaves, focus=imagenet_class, random_crop=True)\n",
    "    img_fn = '_'.join([model_name, \"deepdraw_denoised\", str(imagenet_class)+'.png'])\n",
    "    PIL.Image.fromarray(cv2.bilateralFilter(np.uint8(gen_image), -1, 25, 6)).save('./' + img_fn)\n",
    "    # PIL.Image.fromarray(np.uint8(gen_image)).save('./' + img_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
